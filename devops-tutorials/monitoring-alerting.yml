# Monitoring & Alerting with Prometheus & Grafana

Complete monitoring setup with Prometheus, Grafana, Alertmanager, and AI-powered anomaly detection.

## What This Covers

- üìä **Metrics collection** with Prometheus
- üìà **Visualization** with Grafana
- üö® **Alerting** with Alertmanager
- ü§ñ **AI anomaly detection**
- üìù **Log aggregation**
- üìâ **APM integration**

## File Structure

```
monitoring/
‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îú‚îÄ‚îÄ alertmanager.yml
‚îÇ   ‚îî‚îÄ‚îÄ rules/
‚îÇ       ‚îú‚îÄ‚îÄ cpu.yml
‚îÇ       ‚îú‚îÄ‚îÄ memory.yml
‚îÇ       ‚îî‚îÄ‚îÄ apm.yml
‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.json
‚îÇ   ‚îî‚îÄ‚îÄ datasources.yml
‚îú‚îÄ‚îÄ alertmanager/
‚îÇ   ‚îî‚îÄ‚îÄ config.yml
‚îî‚îÄ‚îÄ node_exporter/
    ‚îî‚îÄ‚îÄ prometheus-node-exporter.yml
```

## Template 1: Prometheus Configuration

`monitoring/prometheus/prometheus.yml`:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    monitor: 'myapp-monitoring'
    environment: 'production'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Rule files
rule_files:
  - "rules/*.yml"

# Scrape configurations
scrape_configs:
  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node Exporter (System metrics)
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    metrics_path: /metrics
    scheme: http
    metrics_path: /metrics
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__address__]
        target: __address__
        regex: (.+)
        target_label: __param_target
        replacement: localhost:9100
        target_label: __param_target
        regex: (.+)
        target_label: __address__
        replacement: localhost:9100

  # Kubelet metrics
  - job_name: 'kubernetes-nodes'
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecure_skip_verify: true
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __address__
        replacement: ${1}:10250

  # Kubelet Metrics (Cadvisor)
  - job_name: 'kubernetes-cadvisor'
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      insecure_skip_verify: true
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        replacement: ${1}:10250
        target_label: __address__
        regex: (.+)

  # Kube-Prometheus-Stack
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: ${1}:${2}
        target_label: __address__
  
  # Application metrics (custom)
  - job_name: 'myapp'
    static_configs:
      - targets: ['myapp:3000']
    metrics_path: /metrics
    scheme: http
    scrape_interval: 15s
    scrape_timeout: 10s

  # Redpanda (or Kafka) metrics
  - job_name: 'redpanda'
    static_configs:
      - targets: ['redpanda:9644']

  # Grafana metrics
  - job_name: 'grafana'
    static_configs:
      - targets: ['grafana:3000']
```

## Template 2: Kubernetes Prometheus Configuration

`monitoring/prometheus/prometheus-k8s.yml`:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
  labels:
    app: prometheus
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https
      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
```

## Template 3: Alert Rules

`monitoring/prometheus/rules/cpu.yml`:
```yaml
groups:
  - name: cpu
    interval: 1m
    rules:
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% on {{ $labels.instance }} (current value: {{ $value }}%)"
      
      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical CPU usage"
          description: "CPU usage is above 95% on {{ $labels.instance }} (current value: {{ $value }}%)"
      
      - alert: CPUThrottlingHigh
        expr: rate(cadvisor_container_cpu_cfs_periods_total[1m]) / rate(cadvisor_container_cpu_cfs_throttled_periods_total[1m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU throttling"
          description: "Container {{ $labels.container }} is experiencing CPU throttling"

      - alert: PodCPUThrottling
        expr: rate(container_cpu_cfs_throttled_seconds_total[5m]) / rate(container_cpu_cfs_seconds_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod CPU throttling detected"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is experiencing CPU throttling ({{ $value | humanizePercentage }} of time throttled)"
```

`monitoring/prometheus/rules/memory.yml`:
```yaml
groups:
  - name: memory
    interval: 1m
    rules:
      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage above 80% on {{ $labels.container }} ({{ $value | humanizePercentage }})"
      
      - alert: OOMKilled
        expr: increase(container_memory_oom_kill_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Container OOMKilled"
          description: "Container {{ $labels.container }} has been OOM killed in the last 5 minutes"
      
      - alert: MemoryLeak
        expr: increase(container_memory_cache_bytes[1h]) / increase(container_memory_usage_bytes[1h]) > 0.9
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Possible memory leak"
          description: "Cache usage growing faster than memory usage in {{ $labels.container }}"

      - alert: PodMemoryExhaustion
        expr: (container_memory_working_set_bytes / container_spec_memory_limit_bytes) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Pod memory running low"
          description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of its memory limit"
```

## Template 4: Alertmanager Configuration

`monitoring/alertmanager/config.yml`:
```yaml
global:
  smtp_smarthost: 'smtp.example.com:587'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'your_password'
  smtp_from: 'alerts@example.com'
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
  pagerduty_url: 'https://events.pagerduty.com/generic/20210307'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 4h
  receiver: 'slack-notifications'
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      continue: true
    - match:
        severity: warning
      receiver: 'slack-warnings'

receivers:
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#alerts'
        send_resolved: true
        title: '‚ö†Ô∏è {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}
{% endrange }}'
  
  - name: 'slack-warnings'
    slack_configs:
      - channel: '#alerts-warning'
        send_resolved: true
        title: '‚ö†Ô∏è {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}
{% endrange }}'
  
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'your-pagerduty-service-key'
        severity: critical
        description: '{{ .GroupLabels.alertname }} - {{ .CommonLabels.alertname }}'

  - name: 'email-admins'
    email_configs:
      - to: 'alerts@example.com'
        send_resolved: true
        html: '{{ template "email.html" . }}'
```

## Template 5: Grafana Datasource Configuration

`monitoring/grafana/datasources.yml`:
```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    jsonData:
      httpMethod: POST
      timeInterval: 15s
  
  - name: CloudWatch
    type: cloudwatch
    access: proxy
    url: 'https://monitoring.{region}.amazonaws.com'
    editable: false
    jsonData:
      defaultRegion: 'us-east-1'
  
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    editable: false
    jsonData:
      derivedFields:
        - datasourceUid: 'Prometheus'
          matcherRegex: '(\w+:\d+/\d+)'
          name: Request
          url: '$${__Value}'
          urlDisplayLabel: 'View in Prometheus'
```

## Template 6: Kubernetes Monitoring Dashboard

```json
{
  "dashboard": {
    "title": "Kubernetes Cluster Monitoring",
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "type": "graph",
        "title": "CPU Usage by Pod",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)",
            "legendFormat": "{{ pod }}",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "label": "CPU (cores)",
            "min": 0
          }
        ],
        "xaxis": {
          "mode": "time"
        }
      },
      {
        "id": 2,
        "type": "graph",
        "title": "Memory Usage by Pod",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "container_memory_working_set_bytes / container_spec_memory_limit_bytes * 100",
            "legendFormat": "{{ pod }}",
            "refId": "A"
          }
        ],
        "percentage": true,
        "yaxes": [
          {
            "label": "Memory %",
            "min": 0,
            "max": 100
          }
        ]
      },
      {
        "id": 3,
        "type": "stat",
        "title": "Pods Running",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "sum(kube_pod_status_ready{condition=\"true\"}) or sum(kube_pod_status_phase{phase=\"Running\"})",
            "legendFormat": "Running Pods",
            "refId": "A"
          }
        ],
        "thresholds": {
          "steps": [
            {"color": "green", "value": null},
            {"color": "yellow", "value": 5},
            {"color": "red", "value": 0}
          ]
        }
      },
      {
        "id": 4,
        "type": "gauge",
        "title": "Cluster Health",
        "datasource": "Prometheus",
        "targets": [
          {
            "expr": "100 - (count(kube_pod_status_phase{phase!=\"Running\", phase!=\"Succeeded\"}) / count(kube_pod_status_phase{phase!=\"Unknown\"}) * 100)",
            "legendFormat": "Cluster Health",
            "refId": "A"
          }
        ],
        "thresholds": {
          "steps": [
            {"color": "green", "value": null},
            {"color": "yellow", "value": 75},
            {"color": "red", "value": 50}
          ]
        }
      }
    ],
    "refresh": "15s",
    "time": {
      "from": "now-1h",
      "to": "now"
    }
  }
}
```

## Deployment with Helm

`helm/values.yaml`:
```yaml
prometheus:
  prometheusSpec:
    retention: 15d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: gp2
          resources:
            requests:
              storage: 50Gi
    
    externalLabels:
      cluster: production
    
    podDisruptionBudget:
      minAvailable: 1
    
    resources:
      requests:
        memory: 2Gi
        cpu: 500m

nodeExporter:
  enabled: true
  fullnameOverride: "node-exporter"

alertmanager:
  enabled: true
  config:
    global:
      smtp_smarthost: 'smtp.example.com:587'
      smtp_auth_username: 'alerts@example.com'
      smtp_from: 'alerts@example.com'
  resources:
    requests:
      memory: 256Mi
      cpu: 100m

grafana:
  enabled: true
  adminPassword: 'secure-password-change-me'
  sidecar:
    datasources:
      enabled: true
      searchNamespace: ALL
    dashboards:
      enabled: true
      searchNamespace: ALL
  resources:
    requests:
      memory: 256Mi
      cpu: 100m

kube-state-metrics:
  enabled: true
  prometheus:
    monitor:
      enabled: true
      namespace: default

cAdvisor:
  enabled: true
  prometheus:
    monitor:
      enabled: true
```

## AI Anomaly Detection Integration

```python
# ai-monitoring/ai_alerts.py
import requests
import numpy as np
from datetime import datetime, timedelta

class AIAnomalyDetector:
    def __init__(self, prometheus_url, api_key):
        self.prometheus_url = prometheus_url
        self.api_key = api_key
        self.headers = {'Authorization': f'Bearer {api_key}'}
    
    def get_metrics(self, query, start_time, end_time):
        """Fetch metrics from Prometheus"""
        url = f"{self.prometheus_url}/api/v1/query_range"
        params = {
            'query': query,
            'start': start_time,
            'end': end_time,
            'step': '60s'
        }
        response = requests.get(url, params=params, headers=self.headers)
        response.raise_for_status()
        return response.json()['data']['result']
    
    def detect_anomaly(self, metrics_data, threshold=2.0):
        """
        Detect anomalies using statistical methods
        Threshold in standard deviations
        """
        values = []
        for point in metrics_data:
            values.append(float(point[1]))
        
        if len(values) < 3:
            return []
        
        # Calculate mean and std
        mean = np.mean(values)
        std = np.std(values)
        
        anomalies = []
        for i, value in enumerate(values):
            z_score = abs((value - mean) / std) if std > 0 else 0
            if z_score > threshold:
                anomalies.append({
                    'time': metrics_data[i][0],
                    'value': value,
                    'z_score': z_score,
                    'deviation': 'above' if value > mean else 'below'
                })
        
        return anomalies
    
    def check_pod_anomalies(self, pod_name, metric='container_cpu_usage_seconds'):
        """Check for anomalies in specific pod metrics"""
        now = datetime.utcnow()
        query = f"sum(rate({{{metric}}}[5m])) by (pod)"
        
        start = (now - timedelta(hours=1)).isoformat()
        end = now.isoformat()
        
        metrics = self.get_metrics(query, start, end)
        anomalies = self.detect_anomaly(metrics)
        
        # Send alert if anomalies found
        if anomalies:
            self.send_alert(pod_name, anomalies)
        
        return anomalies
    
    def send_alert(self, pod_name, anomalies):
        """Send alert notification"""
        alert_payload = {
            'alertname': 'AI Anomaly Detected',
            'pod': pod_name,
            'time': datetime.utcnow().isoformat(),
            'anomalies': anomalies,
            'severity': 'warning'
        }
        
        # Send to Slack
        requests.post(
            'https://hooks.slack.com/services/YOUR/WEBHOOK',
            json=alert_payload,
            headers={'Content-Type': 'application/json'}
        )
        
        print(f"‚ö†Ô∏è Anomaly detected in {pod_name}: {len(anomalies)} anomalies")

# Usage
detector = AIAnomalyDetector(
    prometheus_url='http://prometheus:9090',
    api_key='your-api-key'
)

anomalies = detector.check_pod_anomalies('myapp-pod-123')
if anomalies:
    print(f"Found {len(anomalies)} anomalies!")
```

## Best Practices

1. **Set proper retention** (7-30 days depending on needs)
2. **Use recording rules** to improve query performance
3. **Implement alert suppression** to reduce noise
4. **Tag alerts** with severity and contact info
5. **Use synthetic monitoring** for external endpoints
6. **Monitor the monitoring** (PROMETHEUS_ alerts)
7. **Set up dashboard updates** for new services
8. **Implement rate limiting** on alerts
9. **Use multiple alert channels** for critical alerts
10. **Regular dashboard reviews** and updates

---

*Generated by AI ‚Ä¢ Updated February 2026*
